# Use LLM model API's prefix caching